1
00:00:12,665 --> 00:00:20,060
Что можно сделать, чтобы устранить
негативные последствия автокорреляции?

2
00:00:20,060 --> 00:00:24,710
Одна из стандартных мер,
которая не требует больших усилий от

3
00:00:24,710 --> 00:00:27,711
практика — это исправить
стандартные ошибки.

4
00:00:27,711 --> 00:00:30,127
То есть использовать другую формулу,

5
00:00:30,127 --> 00:00:35,405
а именно формулу для оценки
ковариационной матрицы оценок β с крышкой

6
00:00:35,405 --> 00:00:40,267
состоятельной в условиях
автокорреляции и гетероскедастичности.

7
00:00:40,267 --> 00:00:43,458
HAC расшифровывается как
heteroscedasticity and

8
00:00:43,458 --> 00:00:45,470
autocorrelation consistent.

9
00:00:45,470 --> 00:00:49,412
Следовательно, у нас получатся
другие стандартные ошибки,

10
00:00:49,412 --> 00:00:55,042
и если мы с помощью них будем проверять
гипотезы, то уже асимптотическое

11
00:00:55,042 --> 00:01:00,814
распределение статистики,
t-статистики будет нормальным 0,1,

12
00:01:00,814 --> 00:01:05,053
и мы можем проверять гипотезы и
строить доверительные интервалы.

13
00:01:05,053 --> 00:01:10,691
Робастная (устойчивая) к условной
автокорреляции оценка ковариационной

14
00:01:10,691 --> 00:01:15,565
матрицы была предложена
Нью-Вестом в 1987-м году,

15
00:01:15,565 --> 00:01:19,598
она имеет довольно громоздкую формулу, но,

16
00:01:19,598 --> 00:01:23,712
тем не менее, приятен сам факт,
что она существует,

17
00:01:23,712 --> 00:01:28,370
и мы с помощью компьютера можем ее легко
автоматически посчитать и использовать.

18
00:01:28,370 --> 00:01:34,291
К чему приводит использование формулы
для оценки ковариационной матрицы,

19
00:01:34,291 --> 00:01:35,890
предложенной Нью-Вестом?

20
00:01:35,890 --> 00:01:40,120
С точки зрения практика
все довольно просто.

21
00:01:40,120 --> 00:01:44,350
Мы меняем одни стандартные ошибки на
рассчитываемые по другим формулам,

22
00:01:44,350 --> 00:01:49,907
и использование оценки ковариационной

23
00:01:49,907 --> 00:01:54,782
матрицы Нью-Веста решает проблему
тестирования гипотез об отдельном

24
00:01:54,782 --> 00:01:58,310
коэффициенте и построении доверительных
интервалов для отдельного коэффициента.

25
00:01:58,310 --> 00:02:03,905
То есть t-статистика равная βj с крышкой
минус βj деленное на стандартную ошибку,

26
00:02:03,905 --> 00:02:08,678
устойчивую к автокорреляции теперь
эта t-статистика, с ростом количества

27
00:02:08,678 --> 00:02:13,440
наблюдений, становится все более и более
похожей на нормальное распределение.

28
00:02:13,440 --> 00:02:16,837
И мы получаем возможность строить
доверительные интервалы и

29
00:02:16,837 --> 00:02:18,085
проверять гипотезы.

30
00:02:18,085 --> 00:02:22,730
Какие проблемы использование
этой корректировки не решает?

31
00:02:22,730 --> 00:02:28,057
Во-первых, оценки β с крышкой
остаются неэффективными, и, конечно,

32
00:02:28,057 --> 00:02:33,453
не решаются проблемы с
распределением статистик в конечных

33
00:02:33,453 --> 00:02:38,105
выборках даже с предположением
о нормальности ε.

34
00:02:38,105 --> 00:02:42,086
С практической точки зрения
в R все будет просто.

35
00:02:42,086 --> 00:02:45,740
Мы оцениваем модель как и раньше
методом наименьших квадратов,

36
00:02:45,740 --> 00:02:51,288
то есть с помощью функции lm, и теперь,
чтобы посчитать ковариационную матрицу

37
00:02:51,288 --> 00:02:55,956
оценок коэффициентов, мы используем
команду vcovHAC для данной модели.

38
00:02:55,956 --> 00:03:00,762
И использование правильных
стандартных ошибок решает

39
00:03:00,762 --> 00:03:04,920
проблему с стандартными ошибками
для отдельно взятых коэффициентов.

40
00:03:04,920 --> 00:03:08,720
Когда на практике следует
использовать эту формулу?

41
00:03:08,720 --> 00:03:13,239
Как только вы подразумеваете
наличие зависимости

42
00:03:13,239 --> 00:03:17,718
между близкими наблюдениями
в ваших данных и не хотите

43
00:03:17,718 --> 00:03:22,682
заниматься специально моделированием
этой структуры зависимости.

44
00:03:22,682 --> 00:03:27,706
То есть, если ваши данные представляют
собой временные ряды или ваши данные —

45
00:03:27,706 --> 00:03:32,860
это данные, где имеет место географическая
близость между отдельно взятыми

46
00:03:32,860 --> 00:03:37,851
наблюдениями, во всех этих случаях
имеет смысл использовать ошибки,

47
00:03:37,851 --> 00:03:44,290
стандартные ошибки Нью-Веста, то есть
состоятельные в условиях автокорреляции.

48
00:03:44,290 --> 00:03:47,230
Как можно обнаружить
автокорреляцию в имеющихся данных?

49
00:03:47,230 --> 00:03:49,359
Как и в случае гетероскедастичности,

50
00:03:49,359 --> 00:03:53,630
автокорреляцию можно обнаружить
графически и с помощью формальных тестов.

51
00:03:53,630 --> 00:03:58,859
Для графического обнаружения
автокорреляции оценивают

52
00:03:58,859 --> 00:04:03,861
исходную модель с помощью метода
наименьших квадратов, получают

53
00:04:03,861 --> 00:04:08,790
остатки в исходной модели, то есть ε1
с крышкой, ε2 с крышкой и так далее.

54
00:04:08,790 --> 00:04:11,042
А дальше на графике по одной оси,

55
00:04:11,042 --> 00:04:16,120
по горизонтальной оси откладывают
предыдущий остаток εt–1 с крышкой,

56
00:04:16,120 --> 00:04:20,310
а по вертикали откладывают
следующий остаток εt с крышкой.

57
00:04:20,310 --> 00:04:24,410
Соответственно, если зависимости
между остатками нет,

58
00:04:24,410 --> 00:04:28,840
то облако получающихся точек
будет примерно похоже на круг.

59
00:04:28,840 --> 00:04:33,725
А ежели зависимость между остатками есть,
если εt с крышечкой,

60
00:04:33,725 --> 00:04:38,610
сегодняшний остаток зависит от вчерашнего,
от εt–1 с крышечкой,

61
00:04:38,610 --> 00:04:41,361
то тогда мы увидим облако,

62
00:04:41,361 --> 00:04:47,020
вытянутое из первой четверти
в третью четверть графика.

63
00:04:47,020 --> 00:04:53,871
Если же зависимость между εt с крышечкой
и εt с крышечкой – 1 отрицательная,

64
00:04:53,871 --> 00:04:59,245
то мы, соответственно, увидим облако
точек, вытянутое в другом направлении.

65
00:04:59,245 --> 00:05:02,850
Теперь перейдем к формальным
тестам на автокорреляцию.

66
00:05:02,850 --> 00:05:09,883
Два, пожалуй, самых известных теста — это
тест Дарбина-Уотсона и тест Бройша-Годфри.

67
00:05:09,883 --> 00:05:15,718
Несмотря на большую применимость
теста Бройша-Годфри,

68
00:05:15,718 --> 00:05:19,694
тест Дарбина-Уотсона
достаточно популярный,

69
00:05:19,694 --> 00:05:24,416
поэтому я начну свое изложение с него,
хотя, конечно, он уступает

70
00:05:24,416 --> 00:05:29,280
тесту Бройша-Годфри по своим возможностям,
то есть по предпосылкам использования.

71
00:05:29,280 --> 00:05:33,031
Тест Дарбина-Уотсона гораздо более
ограничительный и требует выполнения

72
00:05:33,031 --> 00:05:37,436
гораздо большего числа необходимых
предпосылок, чем тест Бройша-Годфри.

73
00:05:37,436 --> 00:05:39,340
Итак, тест Дарбина-Уотсона.

74
00:05:39,340 --> 00:05:43,436
Во-первых, тест Дарбина-Уотсона
предназначен только для тестирования

75
00:05:43,436 --> 00:05:48,291
автокорреляций первого порядка,
то есть для тестирования гипотезы в рамках

76
00:05:48,291 --> 00:05:52,392
предположения εt = ρ * εt–1 + ut мы в

77
00:05:52,392 --> 00:05:57,230
рамках этого предположения тестируем
гипотезу о том, что ρ = 0.

78
00:05:57,230 --> 00:06:00,469
При этом предполагается
нормальность ошибок ε,

79
00:06:00,469 --> 00:06:03,669
предполагается сильная
экзогенность ошибок,

80
00:06:03,669 --> 00:06:09,200
то есть математическое ожидание от εt
при фиксированном X должно равняться 0.

81
00:06:09,200 --> 00:06:14,072
И мы проверяем с помощью него нулевую
гипотезу об отсутствии автокорреляции,

82
00:06:14,072 --> 00:06:15,680
то есть о том, что ρ = 0.

83
00:06:15,680 --> 00:06:19,720
Процедура теста Дарбина-Уотсона следующая.

84
00:06:19,720 --> 00:06:24,341
Шаг 1 — оценить исходную
модель с помощью МНК и

85
00:06:24,341 --> 00:06:29,700
получить из исходной модели
остатки ε с крышечкой.

86
00:06:29,700 --> 00:06:35,798
И второй шаг — посчитать статистику
Дарбина-Уотсона по формуле: дробь,

87
00:06:35,798 --> 00:06:42,794
в числителе сумма квадратов
разностей εi – εi–1 с крышечками,

88
00:06:42,794 --> 00:06:47,160
и в знаменателе просто
сумма квадратов остатков.

89
00:06:47,160 --> 00:06:49,422
К сожалению,

90
00:06:49,422 --> 00:06:54,993
распределение статистики Дарбина-Уотсона,
даже при выполнении

91
00:06:54,993 --> 00:06:59,530
гипотезы H0 об отсутствии автокорреляции
является довольно неприятным.

92
00:06:59,530 --> 00:07:02,470
Оно сложным образом
нетривиальным зависит от X.

93
00:07:02,470 --> 00:07:07,117
К счастью, можно сделать некоторые простые
качественные выводы, основываясь на том,

94
00:07:07,117 --> 00:07:14,090
что статистика Дарбина-Уотсона примерно
равна 2 *(1 – оценку корреляций ρ).

95
00:07:14,090 --> 00:07:19,815
Поэтому, качественные выводы получаются
следующие: если статистика Дарбина-Уотсона

96
00:07:19,815 --> 00:07:24,517
около нуля, это означает корреляцию ρ
с крышечкой, оцененную около единички.

97
00:07:24,517 --> 00:07:29,897
То есть статистика Дарбина-Уотсона около
нуля означает сильную зависимость между

98
00:07:29,897 --> 00:07:34,860
сегодняшним ε и предыдущим,
сильную положительную зависимость.

99
00:07:34,860 --> 00:07:40,945
Статистика Дарбина-Уотсона около 2
означает отсутствие автокорреляции.

100
00:07:40,945 --> 00:07:45,835
И статистика Дарбина-Уотсона около 4
означает сильную отрицательную корреляцию,

101
00:07:45,835 --> 00:07:49,641
примерно равную –1 между
сегодняшним ε и вчерашним.

102
00:07:49,641 --> 00:07:54,315
Несмотря на сложный закон распределения
статистики Дарбина-Уотсона,

103
00:07:54,315 --> 00:07:56,220
на практике все довольно просто.

104
00:07:56,220 --> 00:07:58,349
Статистические пакеты, такие как R,

105
00:07:58,349 --> 00:08:02,973
автоматически рассчитывают P-значение
для теста Дарбина-Уотсона, поэтому мы

106
00:08:02,973 --> 00:08:07,267
просто можем сравнить посчитанное в
R P-значение с уровнем значимости.

107
00:08:07,267 --> 00:08:11,599
Если оно оказывается меньше
5 % посчитанное P-значение,

108
00:08:11,599 --> 00:08:16,323
то мы отвергаем гипотезу об
отсутствии автокорреляции и таким

109
00:08:16,323 --> 00:08:20,810
образом признаем наличие автокорреляции,
а если P-значение велико,

110
00:08:20,810 --> 00:08:25,730
больше 5 %, то мы не отвергаем
гипотезу об отсутствии автокорреляции.

111
00:08:25,730 --> 00:08:30,652
Ну и существуют таблицы,
в которых указаны границы для

112
00:08:30,652 --> 00:08:35,457
критического значения статистики
Дарбина-Уотсона, но эти таблицы,

113
00:08:35,457 --> 00:08:38,380
скорее, представляют исторический интерес.

114
00:08:38,380 --> 00:08:42,986
А сейчас мы рассмотрим второй тест,
менее требовательный по предпосылкам,

115
00:08:42,986 --> 00:08:48,515
чем тест Дарбина-Уотсона,
поэтому более распространенный, а именно

116
00:08:48,515 --> 00:08:53,590
тест Бройша-Годфри.

